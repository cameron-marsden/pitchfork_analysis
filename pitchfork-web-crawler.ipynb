{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitchfork Review Web Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tend to gravitate toward Pitchfork for music reviews because of their unique 0.0-10.0 rating scale. When I approach new music from a critical perspective, there is a massive difference between a score of 8.6 and 9.4. However, for most music review sites, those scores would both round to 9/10 or four-and-a-half stars. \n",
    "\n",
    "So when I first thought about diving into a fun music dataset, I thought it would be interesting to look into Pitchfork's reviews—particularly to see how the publication has changed since it began in the late 90s. I found a few existing datasets, but none of them had all the features I wanted, so I built this web crawler to scrape data directly from Pitchfork. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary libraries for scraping webpages and structuring data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "import urllib.request as ur #Handles URLs\n",
    "from bs4 import BeautifulSoup #Parses webpage content\n",
    "import requests\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Store all Pitchfork pages with album reviews\n",
    "\n",
    "All Pitchfork albums are located under the base URL http://pitchfork.com/reviews/albums/?page=. When I ran my web crawler, there were 1741 pages with 12 reviews on each page. For this step, I simply needed to get a list of all page URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the base URL used for all Pitchfork reviews\n",
    "base_url = \"http://pitchfork.com/reviews/albums/?page=\"\n",
    "\n",
    "#Initialize page counter\n",
    "page_num = 1\n",
    "\n",
    "#Initialize list of pages\n",
    "page_li = []\n",
    "\n",
    "#Populate currently empty list of pitchfork review index pages\n",
    "while page_num < 1741: #or however many pages are available/necessary\n",
    "    #Convert page number to string to be appended to url\n",
    "    page_num_str = str(page_num)\n",
    "    #Create url of index page to scrape for review page urls\n",
    "    full_url = base_url + page_num_str\n",
    "    #Append to list of review page urls to scrape\n",
    "    page_li.append(full_url)\n",
    "    #Bump counter\n",
    "    page_num = page_num + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Store all review URLs\n",
    "\n",
    "Once I had my list of review pages, I needed to get the 12 individual review URLs from each. Web scraping will depend on how the webpage organizes tags. Looking at my browser's console, I found that I needed to get the *fragment_list* class and all of its 12 *a href* tags. Adding a simple if statement checking that the URL includes the string \"/reviews/albums/\" and not \"?genre,\" and I had all the review URLs. This part takes quite a bit of time, so I also added a print statement to let me know when the loop finished adding chunks of 50 URLs to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list of album review page urls\n",
    "review_urls=[]\n",
    "\n",
    "#Initialize iteration count to keep track of the scraper's progress\n",
    "current_iteration = 0\n",
    "\n",
    "#For each item in the list of review index pages do following:\n",
    "for item in page_li:\n",
    "    #Open the review index page and find anything with \"a\" tag used in links\n",
    "    full_url_req = ur.Request(item)\n",
    "    full_url_response = ur.urlopen(full_url_req)\n",
    "    soup = BeautifulSoup(full_url_response)\n",
    "    fragment_lists = soup.find(\"div\", class_=\"fragment-list\")\n",
    "    a_tags = fragment_lists.find_all(\"a\")\n",
    "    for a_tag in a_tags:\n",
    "        #Grab the link in the \"href\" in \"a\" tag\n",
    "        current_href=a_tag.get('href')\n",
    "        #There are two criteria for it being a review page: 1) includes string \"/reviews/albums/\" and longer than 22 characters\n",
    "        if current_href[0:16] == \"/reviews/albums/\" and current_href[16:22] != \"?genre\" and len(current_href) > 22:\n",
    "            list_url = \"http://pitchfork.com\" + current_href\n",
    "            #Append to list of review page urls\n",
    "            review_urls.append(list_url)\n",
    "    \n",
    "    #Report completion for every fifty iterations\n",
    "    if current_iteration % 50 == 0 and current_iteration != 0:\n",
    "        print(\"Completed iteration \" + str(current_iteration))\n",
    "    \n",
    "    #Bump iteration\n",
    "    current_iteration = current_iteration + 1\n",
    "\n",
    "#Print completion\n",
    "print(\"Completed all iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Extract all necessary information from each review\n",
    "\n",
    "Now that I had all the review URLs, I needed to extract the data I wanted from each article:\n",
    "\n",
    "* Full Review\n",
    "* Review Summary\n",
    "* Review Date\n",
    "* Review Score\n",
    "* Critic's Name\n",
    "* Critic's Title\n",
    "* Album Genre\n",
    "* Album Label\n",
    "* Album Artist\n",
    "* Album Name\n",
    "* Whether or not the album received a Best New Music/Best New Reissue Stamp\n",
    "* Album Release Year\n",
    "\n",
    "Once again, I found the HTML tags within my browser's console. Unfortunately, not all review URLs had every piece of information—especially for earlier years. In order to avoid errors, I added try-except logic for every piece. Finally, I added another print function to put my mind at ease while the code looped through each URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize outlist\n",
    "outlist = []\n",
    "\n",
    "#Initialize iteration count\n",
    "current_iteration = 0\n",
    "\n",
    "for url in review_urls:\n",
    "    #Request a url from list of review page urls and get text\n",
    "    resp = requests.get(url)\n",
    "    doc = html.fromstring(resp.text)\n",
    "\n",
    "    #Gather all important aspects of the review\n",
    "    \n",
    "    #Review Body\n",
    "    try:\n",
    "        review_body = doc.find('.//div[@class=\"contents dropcap\"]').text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        review_body = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Review Summary\n",
    "    try:\n",
    "        review_summary = doc.find('.//div[@class=\"review-detail__abstract\"]').text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        review_summary = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Review Date\n",
    "    try:\n",
    "        review_date = doc.find(\".//time[@class='pub-date']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        review_date = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Review Score\n",
    "    try:\n",
    "        review_score = doc.find(\".//span[@class='score']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        review_score = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Article Author's Name\n",
    "    try:\n",
    "    #Author Information\n",
    "        author_name = doc.find(\".//a[@class='authors-detail__display-name']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        author_name = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Article Author's Position/Title\n",
    "    try:\n",
    "        author_title = doc.find(\".//span[@class='authors-detail__title']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        author_title = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Album Genre\n",
    "    try:\n",
    "        album_genre = doc.find(\".//li[@class='genre-list__item']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_genre = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Album Label\n",
    "    try:\n",
    "        album_label = doc.find(\".//li[@class='labels-list__item']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_label = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Album Artist\n",
    "    try:\n",
    "        album_artist = doc.find(\".//ul[@class='artist-links artist-list single-album-tombstone__artist-links']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_artist = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Album Name\n",
    "    try:\n",
    "        album_title = doc.find(\".//h1[@class='single-album-tombstone__review-title']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_title = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Best New Music/Reissue Tag\n",
    "    try:\n",
    "        album_bnm = doc.find(\".//p[@class='bnm-txt']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_bnm = \"Not Available\".encode('utf-8')\n",
    "    \n",
    "    #Album Release Year\n",
    "    try:\n",
    "        album_year = doc.find(\".//span[@class='single-album-tombstone__meta-year']\").text_content().strip().encode('utf-8')\n",
    "    except:\n",
    "        album_year = \"Not Available\".encode('utf-8')\n",
    "        \n",
    "    #Put current loop's information in a list\n",
    "    inlist = [album_artist,album_title,album_genre,album_label,album_bnm,album_year,\n",
    "              author_name,author_title,\n",
    "              review_summary,review_body,review_date,review_score,url]\n",
    "    \n",
    "    #Append current album information to outlist\n",
    "    outlist.append(inlist)\n",
    "    \n",
    "    #Report completion for every fifty iterations\n",
    "    if current_iteration % 50 == 0 and current_iteration != 0:\n",
    "        print(\"Completed iteration \" + str(current_iteration))\n",
    "    \n",
    "    #Bump iteration count\n",
    "    current_iteration = current_iteration + 1\n",
    "\n",
    "#Print completion\n",
    "print(\"Completed all iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a data table with all scraped information\n",
    "\n",
    "When my loop finished scraping data, I simply needed to put the information in a Pandas dataframe and decode all UTF-8 encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from outlist\n",
    "df = pd.DataFrame(outlist, columns = [\"Artist\", \"Album\", \"Genre\", \"Label\", \"BNM\", \"Year\",\"Author\", \"Title\",\n",
    "                                      \"Summary\", \"Review\", \"Date\", \"Score\", \"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode all columns except URL\n",
    "str_df = df.select_dtypes([np.object]).drop('URL', axis=1)\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "\n",
    "for col in str_df:\n",
    "    df[col] = str_df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Cleansing the Raw Data\n",
    "\n",
    "Now that I had the raw data, I needed to do a little maintenance with the date-related variables. Reissued albums had year values that showed both release and review years, so I split them into two separate features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace bullet point with Not Available\n",
    "df.Year[df.Year==\"•\"] = \"Not Available\"\n",
    "\n",
    "#Get Original_Year and Reissue_Year variables\n",
    "df['Original_Year'] = df.Year.str.partition('/')[0]\n",
    "df['Reissue_Year'] =  df.Year.str.partition('/')[2]\n",
    "\n",
    "#Designate whitespaces as missings\n",
    "df['Reissue_Year'] = df['Reissue_Year'].replace(r'\\s+|^$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Designate likely new albums vs. reissues\n",
    "\n",
    "Pitchfork reviews plenty of reissued albums, and they have recently started reviewing impactful older albums on Sundays. I knew I would probably want to filter the data to new album reviews for my analysis, so I created some rules to indicate which reviews likely corresponded with new vs. old albums.\n",
    "\n",
    "* If the album has Year=\"Not Available,\" it is not new. (Compilation of prior releases)\n",
    "* If the album has a Reissue_Year, it is not new.\n",
    "* If the album has Year < Review Year - 1, it is not new\n",
    "* If the album name contains \"anniversary edition,\" it is not new\n",
    "* If BNM is \"Best New Reissue,\" it is not new\n",
    "* If BNM is \"Best New Music,\" it is new\n",
    "* If the album summary or review includes the word \"reissue\" it is not new\n",
    "* The remainder are considered new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get date\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d %Y')\n",
    "df['Review_Year'] = df['Date'].dt.year\n",
    "\n",
    "#Create a variable to denote the difference between when the album was released vs when it was reviewed\n",
    "df['Original_Year'] = pd.to_numeric([np.NaN if x == 'Not Available' or len(x) != 4 else x for x in df['Original_Year']])\n",
    "df['Review_Release_Difference'] = df['Review_Year'] - df['Original_Year']\n",
    "\n",
    "#Was the album released two or more years before the review?\n",
    "df['Large_Time_Gap'] = [0 if x in range(0,2) else 1 for x in df['Review_Release_Difference']]\n",
    "\n",
    "#Does the album name contain \"anniversary edition\"?\n",
    "df['Anniversary_Edition'] = pd.to_numeric(df['Album'].str.contains('anniversary edition', case=False, regex=False))\n",
    "\n",
    "#Does the album have \"Best new reissue\" tag?\n",
    "df['Best_New_Reissue'] = [1 if x == 'Best new reissue' else 0 for x in df['BNM']]\n",
    "\n",
    "#Does the summary include the word 'Reissue'\n",
    "df['Summary_Contains_Reissue'] = pd.to_numeric(df['Summary'].str.contains('reissue', case=False))\n",
    "\n",
    "#Does the review include the word 'Reissue'\n",
    "df['Review_Contains_Reissue'] = pd.to_numeric(df['Review'].str.contains('reissue', case=False))\n",
    "\n",
    "#Create a variable showing the number of rules broken\n",
    "df['Broken_Rules_Count'] = df['Large_Time_Gap'] + df['Anniversary_Edition'] + df['Best_New_Reissue'] + df['Summary_Contains_Reissue'] + df['Review_Contains_Reissue']\n",
    "\n",
    "#Create New_Album variable indicating whether any rules were broken\n",
    "df['New_Album'] = [1 if x in range(0,1) else 0 for x in df['Broken_Rules_Count']]\n",
    "\n",
    "#Ensure albums with \"Best new music\" are set as new\n",
    "df['New_Album'] = np.where(df['BNM']=='Best new music', 1, df['New_Album'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Store Cleansed Dataset\n",
    "\n",
    "Now we have a nice dataset to use for analysis! You can find the pickle file at https://drive.google.com/open?id=1dnlZn10IrZHCG3drdf_U8XymNWe5OqQZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write dataset to pickle\n",
    "df.to_pickle(\"p4k_clenased.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
